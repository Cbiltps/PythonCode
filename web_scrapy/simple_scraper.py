"""
ç®€åŒ–ç‰ˆç½‘é¡µçˆ¬å–å·¥å…·
ç›´æ¥ä½¿ç”¨requestsåº“çˆ¬å–ç½‘é¡µHTMLå†…å®¹å¹¶ä¿å­˜
"""

import os
from urllib.parse import urlparse, urljoin
import re

from requests import get, exceptions
import zstandard as zstd
from bs4 import BeautifulSoup
from markdownify import markdownify as md

# URL é…ç½®å…¥å£ï¼šå°†è¦çˆ¬å–çš„ç½‘ç«™å†™åœ¨è¿™é‡Œ
TARGET_URL_SIMPLE = (
    "https://www.runoob.com/java/java-override-overload.html"  # ä¿®æ”¹ä¸ºä½ è¦çˆ¬å–çš„ç½‘é¡µURL
)


def clean_filename(title, url):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦"""
    if not title or title.strip() == "":
        # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨åŸŸå
        parsed_url = urlparse(url)
        title = parsed_url.netloc.replace("www.", "")

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    safe_title = "".join(
        c for c in title if c.isalnum() or c in (" ", "-", "_", ".")
    ).strip()

    # é™åˆ¶é•¿åº¦
    if len(safe_title) > 100:
        safe_title = safe_title[:100]

    return safe_title.replace(" ", "_")


def download_image(img_url, save_dir, index, headers):
    """
    ä¸‹è½½å•ä¸ªå›¾ç‰‡

    Args:
        img_url (str): å›¾ç‰‡URL
        save_dir (str): ä¿å­˜ç›®å½•
        index (int): å›¾ç‰‡åºå·ï¼ˆç”¨äºå»é‡æ—¶çš„åç¼€ï¼‰
        headers (dict): è¯·æ±‚å¤´

    Returns:
        str: æˆåŠŸè¿”å›æœ¬åœ°æ–‡ä»¶åï¼Œå¤±è´¥è¿”å›None
    """
    try:
        response = get(img_url, headers=headers, timeout=30)
        response.raise_for_status()

        # ä»URLä¸­æå–åŸå§‹æ–‡ä»¶å
        parsed_url = urlparse(img_url)
        original_filename = os.path.basename(parsed_url.path)
        
        # å¦‚æœURLè·¯å¾„æ²¡æœ‰æ–‡ä»¶åï¼Œä½¿ç”¨é»˜è®¤åç§°
        if not original_filename or '.' not in original_filename:
            # ä»Content-Typeè·å–æ‰©å±•å
            content_type = response.headers.get("Content-Type", "")
            if "jpeg" in content_type or "jpg" in content_type:
                ext = ".jpg"
            elif "png" in content_type:
                ext = ".png"
            elif "gif" in content_type:
                ext = ".gif"
            elif "webp" in content_type:
                ext = ".webp"
            else:
                ext = ".jpg"
            original_filename = f"image_{index:03d}{ext}"
        else:
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
            original_filename = clean_filename(original_filename, img_url)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ åºå·
        filepath = os.path.join(save_dir, original_filename)
        if os.path.exists(filepath):
            name, ext = os.path.splitext(original_filename)
            original_filename = f"{name}_{index}{ext}"
            filepath = os.path.join(save_dir, original_filename)

        with open(filepath, "wb") as f:
            f.write(response.content)

        print(f"  âœ“ ä¸‹è½½å›¾ç‰‡: {original_filename} ({len(response.content)} bytes)")
        return original_filename  # è¿”å›æ–‡ä»¶å

    except Exception as e:
        print(f"  âœ— ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
        return None


def extract_and_download_images(html_content, base_url, headers):
    """
    ä»HTMLä¸­æå–å¹¶ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼Œä¿æŒå›¾ç‰‡é¡ºåºå’Œä½ç½®

    Args:
        html_content (str): HTMLå†…å®¹
        base_url (str): ç½‘é¡µåŸºç¡€URL
        headers (dict): è¯·æ±‚å¤´

    Returns:
        tuple: (æˆåŠŸä¸‹è½½çš„å›¾ç‰‡æ•°é‡, å›¾ç‰‡URLåˆ°æœ¬åœ°æ–‡ä»¶åçš„æ˜ å°„å­—å…¸)
    """
    # åˆ›å»ºimagesæ–‡ä»¶å¤¹
    images_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "images")
    os.makedirs(images_dir, exist_ok=True)

    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, "html.parser")

    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
    img_tags = soup.find_all("img")
    print(f"\nğŸ” å‘ç° {len(img_tags)} ä¸ªå›¾ç‰‡æ ‡ç­¾")

    # ä½¿ç”¨åˆ—è¡¨ä¿æŒå›¾ç‰‡é¡ºåºï¼Œä½¿ç”¨å­—å…¸å»é‡
    img_url_to_filename = {}  # URL -> æœ¬åœ°æ–‡ä»¶åçš„æ˜ å°„
    img_urls_ordered = []  # æŒ‰å‡ºç°é¡ºåºçš„URLåˆ—è¡¨

    # ä»imgæ ‡ç­¾æå–URLï¼Œä¿æŒé¡ºåº
    for img in img_tags:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„å±æ€§
        for attr in ["src", "data-src", "data-original", "data-lazy-src"]:
            img_url = img.get(attr)
            if img_url and isinstance(img_url, str):
                # è½¬æ¢ä¸ºç»å¯¹URL
                absolute_url = urljoin(base_url, img_url)
                # å¦‚æœè¿™ä¸ªURLè¿˜æ²¡æœ‰è¢«è®°å½•ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                if absolute_url not in img_url_to_filename:
                    img_urls_ordered.append(absolute_url)
                    img_url_to_filename[absolute_url] = None  # å…ˆå ä½
                break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆå±æ€§åå°±é€€å‡º

    print(f"ğŸ“¸ å…±æ‰¾åˆ° {len(img_urls_ordered)} ä¸ªå”¯ä¸€å›¾ç‰‡URLï¼ˆæŒ‰HTMLä¸­å‡ºç°é¡ºåºï¼‰")

    if not img_urls_ordered:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡")
        return 0, {}

    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡ï¼Œä¿æŒé¡ºåº
    success_count = 0
    for index, img_url in enumerate(img_urls_ordered, 1):
        # ä¸‹è½½å›¾ç‰‡å¹¶è·å–æœ¬åœ°æ–‡ä»¶å
        local_filename = download_image(img_url, images_dir, index, headers)
        if local_filename:
            img_url_to_filename[img_url] = local_filename
            success_count += 1

    return success_count, img_url_to_filename


def convert_html_to_markdown_with_local_images(html_content, base_url, img_url_mapping, output_path):
    """
    å°†HTMLè½¬æ¢ä¸ºMarkdownï¼Œå¹¶å°†å›¾ç‰‡URLæ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„

    Args:
        html_content (str): HTMLå†…å®¹
        base_url (str): ç½‘é¡µåŸºç¡€URL
        img_url_mapping (dict): å›¾ç‰‡URLåˆ°æœ¬åœ°æ–‡ä»¶åçš„æ˜ å°„
        output_path (str): è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„

    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    try:
        # ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œæ›¿æ¢å›¾ç‰‡URL
        soup = BeautifulSoup(html_content, "html.parser")
        
        # æ›¿æ¢æ‰€æœ‰imgæ ‡ç­¾çš„srcå±æ€§
        img_tags = soup.find_all("img")
        replaced_count = 0
        
        for img in img_tags:
            # å°è¯•å¤šä¸ªå¯èƒ½çš„å±æ€§
            for attr in ["src", "data-src", "data-original", "data-lazy-src"]:
                img_url = img.get(attr)
                if img_url and isinstance(img_url, str):
                    # è½¬æ¢ä¸ºç»å¯¹URL
                    absolute_url = urljoin(base_url, img_url)
                    
                    # å¦‚æœè¿™ä¸ªURLåœ¨æ˜ å°„ä¸­ï¼Œæ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
                    if absolute_url in img_url_mapping and img_url_mapping[absolute_url]:
                        local_filename = img_url_mapping[absolute_url]
                        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼š../images/filename
                        local_path = f"../images/{local_filename}"
                        img["src"] = local_path
                        # æ¸…é™¤å…¶ä»–æ‡’åŠ è½½å±æ€§
                        for remove_attr in ["data-src", "data-original", "data-lazy-src"]:
                            if img.get(remove_attr):
                                del img[remove_attr]
                        replaced_count += 1
                        break
        
        print(f"\nğŸ”„ æ›¿æ¢äº† {replaced_count} ä¸ªå›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„")
        
        # å°†ä¿®æ”¹åçš„HTMLè½¬æ¢ä¸ºMarkdown
        modified_html = str(soup)
        markdown_content = md(modified_html, heading_style="ATX")
        
        # ä¿å­˜Markdownæ–‡ä»¶
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        
        print(f"âœ… æˆåŠŸè½¬æ¢ä¸ºMarkdown: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢Markdownå¤±è´¥: {e}")
        return False


def scrape_simple(url):
    """
    ç®€å•çš„ç½‘é¡µçˆ¬å–åŠŸèƒ½ï¼ŒåŒæ—¶ä¸‹è½½é¡µé¢ä¸­çš„æ‰€æœ‰å›¾ç‰‡

    Args:
        url (str): è¦çˆ¬å–çš„ç½‘ç«™URL
    """
    # åˆ›å»ºhtmlæ–‡ä»¶å¤¹
    html_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "html")
    os.makedirs(html_dir, exist_ok=True)

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

    try:
        print(f"æ­£åœ¨çˆ¬å–: {url}")

        # å‘é€è¯·æ±‚
        response = get(url, headers=headers, timeout=30)
        response.raise_for_status()

        # æŸ¥çœ‹å“åº”å¤´ä¸­çš„ Content-Encoding
        encoding = response.headers.get("Content-Encoding")
        print("Content-Encoding: ", encoding)

        # è·å–HTMLå†…å®¹
        html_content = None
        # æ ¹æ®ç¼–ç ç±»å‹è¿›è¡Œè§£å‹ç¼©å¤„ç†
        if encoding == "gzip":
            html_content = response.text
        elif encoding == "deflate":
            html_content = response.text
        elif encoding == "br":
            html_content = response.text
        elif encoding == "zstd":
            dctx = zstd.ZstdDecompressor()
            decompressed_data = dctx.decompress(response.content)
            html_content = decompressed_data.decode("utf-8")
        else:
            html_content = response.text

        # å°è¯•ä»HTMLä¸­æå–æ ‡é¢˜
        title = ""
        if "<title>" in html_content and "</title>" in html_content:
            start = html_content.find("<title>") + 7
            end = html_content.find("</title>")
            title = html_content[start:end].strip()

        # ç”Ÿæˆæ–‡ä»¶å
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.replace("www.", "")

        if title:
            filename = f"{domain}_{clean_filename(title, url)}.html"
        else:
            filename = f"{domain}_page.html"

        # ä¿å­˜HTMLæ–‡ä»¶
        file_path = os.path.join(html_dir, filename)

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        print(f"âœ… æˆåŠŸä¿å­˜HTMLæ–‡ä»¶: {file_path}")
        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {title if title else 'æ— æ ‡é¢˜'}")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦")

        # ä¸‹è½½é¡µé¢ä¸­çš„æ‰€æœ‰å›¾ç‰‡
        print("\nğŸ–¼ï¸  å¼€å§‹ä¸‹è½½å›¾ç‰‡...")
        img_count, img_url_mapping = extract_and_download_images(html_content, url, headers)
        print(f"\nâœ… æˆåŠŸä¸‹è½½ {img_count} å¼ å›¾ç‰‡åˆ° images/ æ–‡ä»¶å¤¹")

        # è½¬æ¢ä¸ºMarkdown
        print("\nğŸ“ å¼€å§‹è½¬æ¢ä¸ºMarkdown...")
        markdown_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "markdown")
        os.makedirs(markdown_dir, exist_ok=True)
        
        # ç”ŸæˆMarkdownæ–‡ä»¶åï¼ˆä¸HTMLåŒåï¼Œä½†æ‰©å±•åä¸º.mdï¼‰
        md_filename = filename.replace(".html", ".md")
        md_path = os.path.join(markdown_dir, md_filename)
        
        convert_html_to_markdown_with_local_images(html_content, url, img_url_mapping, md_path)

        return True

    except exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
        print()
        return False
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False


def main():
    """ä¸»å‡½æ•°ï¼ˆæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼Œç›´æ¥åœ¨ä»£ç ä¸­é…ç½® URLï¼‰"""
    url = TARGET_URL_SIMPLE

    if not url:
        print("è¯·åœ¨ä»£ç ä¸­è®¾ç½® TARGET_URL_SIMPLEï¼Œä¾‹å¦‚ 'https://example.com'")
        return

    # æ£€æŸ¥URLæ ¼å¼
    if not url.startswith(("http://", "https://")):
        url = "https://" + url
        print(f"è‡ªåŠ¨æ·»åŠ åè®®: {url}")

    success = scrape_simple(url)

    if success:
        print("\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("ğŸ“ HTMLæ–‡ä»¶å·²ä¿å­˜åˆ° html/ æ–‡ä»¶å¤¹")
        print("ğŸ–¼ï¸  å›¾ç‰‡æ–‡ä»¶å·²ä¿å­˜åˆ° images/ æ–‡ä»¶å¤¹")
        print("ğŸ“ Markdownæ–‡ä»¶å·²ä¿å­˜åˆ° markdown/ æ–‡ä»¶å¤¹ï¼ˆå›¾ç‰‡é“¾æ¥å·²æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼‰")
    else:
        print("ğŸ’¥ çˆ¬å–å¤±è´¥ï¼")


if __name__ == "__main__":
    main()
